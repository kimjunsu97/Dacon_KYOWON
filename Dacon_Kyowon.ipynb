{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3038b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962ef679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79e3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_HEIGHT_SIZE':64, #64\n",
    "    'IMG_WIDTH_SIZE':224, #224\n",
    "    'EPOCHS':80,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':256,\n",
    "    'NUM_WORKERS':4, # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95321bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a218f797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gsc/dacon_KYOWON/Dacon_KYOWON'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00113144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window\n",
    "# base_dir = 'D:/Dacon_KYOWON/Dacon_KYOWON'\n",
    "# data_dir = \"D:/Dacon_KYOWON/open\"\n",
    "#ubuntu\n",
    "base_dir = '/home/gsc/dacon_KYOWON/Dacon_KYOWON'\n",
    "data_dir = '/home/gsc/dacon_KYOWON/open'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fddd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(f'{data_dir}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7398b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_len_count = pd.DataFrame(df['len'].value_counts())\n",
    "# df_len_count.reset_index(inplace=True)\n",
    "# df_len_count.columns = ['len', 'len_count']\n",
    "\n",
    "# display(df_len_count)\n",
    "\n",
    "# len2count = {k:v for k,v in zip(df_len_count['len'], df_len_count['len_count'])}\n",
    "\n",
    "# idx = 76888 \n",
    "# for i in tqdm(range(200000)):\n",
    "#     select1, select2 = random.randint(0, 76887), random.randint(0, 76887)\n",
    "#     imgFile1 = f'../open/train/TRAIN_{select1:05d}.png'\n",
    "#     imgFile2 = f'../open/train/TRAIN_{select2:05d}.png'\n",
    "    \n",
    "#     if df['len'][select1] + df['len'][select2] > 6:\n",
    "#         continue\n",
    "        \n",
    "#     if len2count[df['len'][select1] + df['len'][select2]] > 30000:\n",
    "#         continue\n",
    "    \n",
    "#     # 이미지 읽기\n",
    "#     img1 = cv2.imread(imgFile1, 1);\n",
    "#     img2 = cv2.imread(imgFile2, 1);\n",
    "    \n",
    "#     img1 = cv2.resize(img1,(100,125))\n",
    "#     img2 = cv2.resize(img2,(100,125))\n",
    "    \n",
    "#     addh = cv2.hconcat([img1, img2])\n",
    "    \n",
    "#     new_data = {\n",
    "#         'id' : f\"TRAIN_{idx:06d}\",\n",
    "#         'img_path' : f\"./Cut_mix/TRAIN_{idx:06d}.png\",\n",
    "#         'label' : df['label'][select1] + df['label'][select2],\n",
    "#         'len' : df['len'][select1] + df['len'][select2]\n",
    "#     }\n",
    "    \n",
    "#     df = df.append(new_data, ignore_index=True)\n",
    "#     cv2.imwrite(f\"./Cut_mix/TRAIN_{idx:06d}.png\", addh)\n",
    "#     idx += 1\n",
    "#     len2count[df['len'][select1] + df['len'][select2]] += 1\n",
    "    \n",
    "# df.to_csv('./train_cutmix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672e38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_cutmix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c944b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad963e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e53f4bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143707 30001\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4c55be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c304964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"-\"] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "934ae66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list,transforms=None, train_mode=True):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(f\"{data_dir}/{img_path.split('/')[-2]}/{img_path.split('/')[-1]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "           \n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, text\n",
    "        else:\n",
    "            return image\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f0efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_transform(height,width, state='train'):\n",
    "    if state == 'train':\n",
    "        transform = A.Compose([\n",
    "                                #A.HorizontalFlip(p=0.2),\n",
    "                                #A.VerticalFlip(p=0.2),\n",
    "                                A.Rotate(limit=[-10,10], p=1),\n",
    "                                #A.RandomRotate90(p=0.2),\n",
    "                                A.Resize(height,width),\n",
    "                                #A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                                #A.RandomResizedCrop(height=height, width=width, scale=(0.3, 1.0)),\n",
    "                                #A.ToGray(),\n",
    "                                A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                ToTensorV2(),\n",
    "                                ])\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "                            A.Resize(height,width),\n",
    "                            #A.ToGray(),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            #A.RandomResizedCrop(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE'], scale=(0.3, 1.0)),               \n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34d91f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = resize_transform(CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])\n",
    "test_transform = resize_transform(CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'],'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0a955bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['머' '써' '빈' ... '행동하다익' '옳싫어하다' '손수']\n"
     ]
    }
   ],
   "source": [
    "img_path = train['label'].values\n",
    "print(img_path)\n",
    "#print(f\"{data_dir}/{img_path.split('/')[-2]}/{img_path.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c548e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values,test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97d40466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 64, 224]) ('팬티적어지다', '글쎄요위협', '동그랗다', '항', '생찾아다니다', '야추가', '버려지다', '되게깸', '빚', '걱정되다긴급', '투표전체', '뺏', '통증옥', '될조심스럽다', '휜', '예의', '그리', '퓌', '메일휴', '역사학수만', '변명', '주무시다새끼', '즐거움죔', '푸르다', '참기름곳곳', '공업', '싸조사하다', '에', '쬡', '안기다총', '생신실현되다', '일본고치다', '참석자', '근거다름없다', '손길', '몽', '운동화', '쫄신랑', '녹화코', '정직하다외침', '쥐참가', '나빠지다성적', '석표시하다', '세기', '보내다확실히', '텍스트', '발소주', '흘러나오다믿', '강요하다', '구분지붕', '꿍', '덤', '젬칼국수', '계산하다퓟', '화분', '깩좌우', '던지다출판', '동화책', '넷째', '수집하다', '입원', '불법', '건전하다년생', '몹시위', '창조하다숟', '자율머물다', '밤새다퓜', '시각주저앉다', '검다', '통과하다', '서부', '손님', '금지되다', '홈페이지요금', '있다감상', '술병', '경영하다녕', '눋', '등록하다깜짝', '며', '뗏할아버지', '앙', '쿤끊어지다', '점차다섯째', '유명하다위', '곡', '쉬다내', '주장하다', '평가되다', '가능해지다', '수', '선자연스럽다', '췻비교하다', '첫유의하다', '양식', '빛', '뵤중요', '들이마시다', '포함접시', '상당수음주', '불법츳', '릉결석하다', '형수', '나가다', '관련', '졌갈증', '재능', '들어오다', '오피스텔았', '어쩐지이리', '정', '깻', '회복되다', '늙', '구분되다청년', '그럼애초', '자차이', '수동적현장', '요구하다폣', '편견아가씨', '금액', '뒵양복', '두다예식장', '땠', '도망가다', '실례의미하다', '밝다경향', '채우다', '신체나빠지다', '쇠', '사고챙기다', '복숭아희생', '내아르바이트', '관리', '차츰솥', '관리', '운동하다', '공기적응하다', '만', '뒤집다버튼', '돕면적', '청소하다프로', '인삼차전기', '분', '글쎄요달다', '해', '함께하다', '수많다', '알', '드리다돌', '폭운전기사', '익다해외여행', '려', '차마진료', '윤', '쉬다', '지각초등학교', '늚', '습', '저', '잡급격히', '뛰어놀다멩', '아저씨', '무너지다아까', '한정신', '상대', '고작', '향상', '앱마사지', '활용하다', '설치야구', '인연', '제시되다아예', '구입녹이다', '순간', '암시거두다', '썹', '반장', '불확실하다', '단어', '일상포근하다', '참가똑바로', '밥', '곁', '엡소용없다', '고춧가루', '무', '어떤나뭇가지', '물론', '펙', '앞별명', '정치적홀로', '대화하다', '식용유기획', '혈액오늘', '의식일반인', '고민하다죽다', '쎈', '양', '세다비하다', '아까', '새', '무덥다서클', '가입하다밤색', '특징', '희망하다', '제비', '계획츌', '초청장', '내년침착하다', '톤자라나다', '결혼', '기르다연출', '깔리다도로', '천', '껙', '킬로그램쿼', '인류전문점', '실현되다동서', '마흔', '상대편담기다', '튕알아내다', '묵다초', '부문', '굶다맣', '채', '제외하다내부', '츠', '꿱', '제외하다있다', '정보출입국', '본인', '작은딸', '강물포스터', '므', '좁애인', '병들다오랜만', '시민장모님', '모', '묾', '등구월', '표시생각되다', '가상켕', '가로', '불안비닐봉지', '장마두다', '벽', '김밥', '불리다기혼', '기능', '호주수학', '급히느껴지다', '뒤쪽대처하다', '다시풍', '수입하다', '프로')\n"
     ]
    }
   ],
   "source": [
    "image_batch, text_batch = iter(train_loader).next()\n",
    "print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1070b520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary as summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7f4b41d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# effnet_b6 = models.efficientnet_b6(pretrained=True)\n",
    "# effnet_b6.cuda()\n",
    "# summary_(effnet_b6,(3,64,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7537caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2,2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0,0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1,1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512,512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        convRelu(7, True)  # \n",
    "        cnn.add_module('pooling{0}'.format(4),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # \n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh),\n",
    "            BidirectionalLSTM(nh, nh, nclass))\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        #print(b,c,h,w)\n",
    "\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39804bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 64, 2350])\n"
     ]
    }
   ],
   "source": [
    "crnn = CRNN(64,3,2350,256).cuda()\n",
    "input = torch.Tensor(64,3,64,224).cuda()\n",
    "output = crnn(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fb94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "\n",
    "# class RecognitionModel_eff(nn.Module):\n",
    "#     def __init__(self, num_chars=len(char2idx), rnn_hidden_size=256):\n",
    "#         super(RecognitionModel_eff, self).__init__()\n",
    "#         self.num_chars = num_chars\n",
    "#         self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "#         # CNN Backbone = 사전학습된 resnet18 활용\n",
    "#         # https://arxiv.org/abs/1512.03385\n",
    "#         #effnet = models.efficientnet_b5(pretrained=True)\n",
    "#         #effnet = models.efficientnet_b7(pretrained=True)\n",
    "#         effnet = models.efficientnet_b6(pretrained=True)\n",
    "#         # CNN Feature Extract\n",
    "#         effnet_modules = list(effnet.features)[:-3]\n",
    "#         self.feature_extract = nn.Sequential(\n",
    "#             *effnet_modules,\n",
    "#             #nn.Conv2d(176, 256, kernel_size=(3,6), stride=1, padding=1), #b5\n",
    "#             nn.Conv2d(200, 256, kernel_size=(3,6), stride=1, padding=1), #b6\n",
    "#             #nn.Conv2d(224, 256, kernel_size=(3,6), stride=1, padding=1), #b7\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.linear1 = nn.Linear(1024, rnn_hidden_size)\n",
    "        \n",
    "#         # RNN\n",
    "#         self.rnn = nn.RNN(input_size=rnn_hidden_size, \n",
    "#                             hidden_size=rnn_hidden_size,\n",
    "#                             bidirectional=True, \n",
    "#                             batch_first=True)\n",
    "#         self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n",
    "        \n",
    "#         #LSTM\n",
    "#         self.lstm = nn.LSTM(input_size=rnn_hidden_size,\n",
    "#                            hidden_size= rnn_hidden_size,\n",
    "#                            bidirectional = True,\n",
    "#                            batch_first=True)\n",
    "        \n",
    "#         #GRU\n",
    "#         self.gru = nn.GRU(input_size=rnn_hidden_size,\n",
    "#                            hidden_size= rnn_hidden_size,\n",
    "#                            bidirectional = True,\n",
    "#                            batch_first=True)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # CNN\n",
    "#         x = self.feature_extract(x) # [batch_size, channels, height, width]\n",
    "#         x = x.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "         \n",
    "#         batch_size = x.size(0)\n",
    "#         T = x.size(1)\n",
    "#         x = x.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "#         x = self.linear1(x)\n",
    "#         #print(x.shape)\n",
    "#         # RNN\n",
    "#         #x, hidden = self.rnn(x)\n",
    "        \n",
    "#         #LSTM\n",
    "#         x, (hidden,_) = self.lstm(x)\n",
    "#         #GRU\n",
    "#         #x, (hidden,_) = self.gru(x)\n",
    "        \n",
    "#         output = self.linear2(x)\n",
    "#         output = output.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b5b39b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RecognitionModel_test = RecognitionModel()\n",
    "# RecognitionModel_test.cuda()\n",
    "# summary_(RecognitionModel_test,(3,64,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1125bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RecognitionModel(nn.Module):\n",
    "#     def __init__(self, num_chars=len(char2idx), rnn_hidden_size=256):\n",
    "#         super(RecognitionModel, self).__init__()\n",
    "#         self.num_chars = num_chars\n",
    "#         self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "#         # CNN Backbone = 사전학습된 resnet18 활용\n",
    "#         # https://arxiv.org/abs/1512.03385\n",
    "#         resnet = resnet18(pretrained=True)\n",
    "#         # CNN Feature Extract\n",
    "#         resnet_modules = list(resnet.children())[:-3]\n",
    "#         self.feature_extract = nn.Sequential(\n",
    "#             *resnet_modules,\n",
    "#             nn.Conv2d(256, 256, kernel_size=(3,6), stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.linear1 = nn.Linear(1024, rnn_hidden_size)\n",
    "        \n",
    "#         # RNN\n",
    "#         self.rnn = nn.RNN(input_size=rnn_hidden_size, \n",
    "#                             hidden_size=rnn_hidden_size,\n",
    "#                             bidirectional=True, \n",
    "#                             batch_first=True)\n",
    "#         self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # CNN\n",
    "#         x = self.feature_extract(x) # [batch_size, channels, height, width]\n",
    "#         x = x.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "         \n",
    "#         batch_size = x.size(0)\n",
    "#         T = x.size(1)\n",
    "#         x = x.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "#         x = self.linear1(x)\n",
    "        \n",
    "#         # RNN\n",
    "#         x, hidden = self.rnn(x)\n",
    "        \n",
    "#         output = self.linear2(x)\n",
    "#         output = output.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "662e30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=0) # idx 0 : '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c037d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_batch(text_batch):\n",
    "    # batch 길이 구하기\n",
    "    text_batch_targets_lens = [len(text) for text in text_batch]\n",
    "    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n",
    "    #batch 합치기\n",
    "    text_batch_concat = \"\".join(text_batch)\n",
    "    #batch 글자 숫자로 인코딩\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "    \n",
    "    return text_batch_targets, text_batch_targets_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f65dfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    text_batch_logps = F.log_softmax(text_batch_logits, 2) # [T, batch_size, num_classes]  \n",
    "    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),), \n",
    "                                       fill_value=text_batch_logps.size(0), \n",
    "                                       dtype=torch.int32).to(device) # [batch_size] \n",
    "\n",
    "    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n",
    "    loss = criterion(text_batch_logps, text_batch_targets, text_batch_logps_lens, text_batch_targets_lens)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83f50492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for image_batch, text_batch in tqdm(iter(train_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        _val_loss = validation(model, val_loader, device)\n",
    "        print(f'Epoch : [{epoch}] Train CTC Loss : [{_train_loss:.5f}] Val CTC Loss : [{_val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "        \n",
    "        if best_loss > _val_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c875ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in tqdm(iter(val_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    _val_loss = np.mean(val_loss)\n",
    "    return _val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df5ec423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e506d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = RecognitionModel_eff()\n",
    "# model.eval()\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "# infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(infer_model,'./weight_file/TransOCR_64_80_cutmix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c915a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(infer_model,'./weight_file/b7_40_lstm_cutmix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f907ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = RecognitionModel()\n",
    "# model.eval()\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "# infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../open/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db43201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None,test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            \n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            \n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ceac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ff63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28013dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../open/sample_submission.csv')\n",
    "submit['label'] = predictions\n",
    "submit['label'] = submit['label'].apply(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submission/submission_80_TransOCR_64_cutmix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0057eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit.to_csv('./submission/submission_30_eff_b6_lstm_cutmix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167358d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_text = pd.read_csv('./submission/submission_80_TransOCR_64_cutmix.csv')\n",
    "sub_text.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8300f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutmix사용 혹은 ATTENTION 구조 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a205d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "\n",
    "def inference_ensemble(model_list, test_loader, device):\n",
    "\n",
    "    text_batch_logits = torch.empty(11,256,2350).cuda()\n",
    "\n",
    "    num = len(model_list)\n",
    "    for i in range(num):\n",
    "        model_list[i].eval()\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            for i in range(num):\n",
    "                text_batch_logits +=model_list[i](image_batch)\n",
    "            \n",
    "            text_batch_logits = text_batch_logits/num\n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            \n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "940d22ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'RecognitionModel_eff' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2856536/459637593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#infer_model1 = torch.load('./weight_file/b7_40_lstm_cutmix.pt').cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minfer_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./weight_file/b6_30_lstm_cutmix.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minfer_model3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./weight_file/b5_30_lstm_cutmix.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minfer_model1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfer_model2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfer_model3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'RecognitionModel_eff' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "#infer_model1 = torch.load('./weight_file/b7_40_lstm_cutmix.pt').cuda()\n",
    "infer_model2 = torch.load('./weight_file/b6_30_lstm_cutmix.pt').cuda()\n",
    "infer_model3 = torch.load('./weight_file/b5_30_lstm_cutmix.pt').cuda()\n",
    "model_list = [infer_model1,infer_model2,infer_model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Tensor(64,3,64,224).cuda()\n",
    "output1 = infer_model1(input)\n",
    "print(output1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = infer_model2(input)\n",
    "print(output2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = infer_model3(input)\n",
    "print(output3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = inference_ensemble(model_list, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efc3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_test = pd.read_csv('../open/sample_submission.csv')\n",
    "submit_test['label'] = predictions_test\n",
    "submit_test['label'] = submit['label'].apply(correct_prediction)\n",
    "submit_test.to_csv('./submission/submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdca9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_text = pd.read_csv('./submission/submission_ensemble.csv')\n",
    "sub_text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.empty()\n",
    "b = torch.Tensor([1])\n",
    "\n",
    "a += b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adda254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
